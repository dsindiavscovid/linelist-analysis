{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mumbai legacy linelist data: Clean up "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up these types of columns:\n",
    "- Enums: dump out a list of enum maps, get the canonical values for these and use these enum maps to replace column values\n",
    "- Dates: Use datefinder to get dates\n",
    "- Regex: Use regex for remaining columns\n",
    "- Text (todo, may/may not be required)\n",
    "\n",
    "Assign status to each cell after clean up:\n",
    "\n",
    "Enum/regex columns\n",
    "- GOOD_PARSE: Clean\n",
    "- MISSING_VAL: Value is missing/NA\n",
    "- BAD_PARSE: Needs manual review\n",
    "\n",
    "Date columns\n",
    "- GOOD_PARSE: Clean\n",
    "- MISSING_PARSE: Date is missing/NA\n",
    "- BAD_PARSE: Cannot be parsed, needs manual review\n",
    "- SUSPECTED_PARSE: Ambiguous as it can be parsed as MM/DD or DD/MM, further date processing for suspected dates in another notebook\n",
    "\n",
    "Assign overall status to row as:\n",
    "- GOOD: All values are either GOOD_PARSE or MISSING_VAL\n",
    "- BAD: At least one BAD_PARSE but no SUSPECTED_PARSE\n",
    "- SUSPECTED: Contains at least one SUSPECTED_PARSE\n",
    "\n",
    "Execution: Read data file, set columns (Column Categories section) and dump out enum maps (Creating enum maps section). Get enum maps manually filled up. Set path to enum maps and run remaining parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import datetime as dt\n",
    "import datefinder\n",
    "from dateutil.parser import parse\n",
    "from datetime import timedelta\n",
    "import time\n",
    "\n",
    "from stemming.porter2 import stem\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input files\n",
    "MAP_DIR = '../maps/' # Directory containing enum map CSVs\n",
    "DATA_FILE = '../data/consolidated-wards-new.csv'\n",
    "\n",
    "# Map to canonical values\n",
    "GENDER_LIST_FILE =  MAP_DIR + \"GENDER_LIST_FILE.csv\"\n",
    "WARD_LIST_FILE =  MAP_DIR + \"WARD_LIST_FILE.csv\"\n",
    "DISTRICT_LIST_FILE =  MAP_DIR +\"DISTRICT_LIST_FILE.csv\"\n",
    "FACILITY_LIST_FILE =  MAP_DIR +\"FACILITY_LIST_FILE.csv\"\n",
    "OUTCOME_LIST_FILE =  MAP_DIR +\"OUTCOME_LIST_FILE.csv\"\n",
    "HSTATUS_LIST_FILE =  MAP_DIR +\"HSTATUS_LIST_FILE.csv\"\n",
    "INTERVENTION_LIST_FILE =  MAP_DIR +\"INTERVENTION_LIST_FILE.csv\"\n",
    "RESULT_STATUS_LIST_FILE =  MAP_DIR + \"RESULT_STATUS_LIST_FILE.csv\"\n",
    "BOOL_LIST_FILE =  MAP_DIR + \"BOOL_LIST_FILE.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = pd.read_csv(DATA_FILE, header=0, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Parsing routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_na(x):\n",
    "    \"\"\"Check if a cell value is NA. \n",
    "    This includes the NaN, hyphens, whitespace and strings such as [na, nan, n/a].\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return True\n",
    "    if isinstance(x, str):\n",
    "        if re.match('(-)+', x) or x.isspace() or x.strip().lower() in [\"na\", \"nan\", \"n/a\"]:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple date parsing\n",
    " \n",
    "def order_fix(x,valid_interval): # NOT USED\n",
    "    \"\"\"mm-dd-order fix \"\"\"\n",
    "    valid_start = valid_interval[0]\n",
    "    valid_end = valid_interval[1]\n",
    "    if ((x > valid_end) or (x < valid_start)):\n",
    "        x =dt.date(x.year,x.day,x.month)\n",
    "    if ((x > valid_end) or (x < valid_start)):\n",
    "        x =None    \n",
    "    return x\n",
    "\n",
    "def parse_date_old(x,valid_interval): # NOT USED\n",
    "    status =\"GOOD_PARSE\"\n",
    "    result = x\n",
    "    if not result:\n",
    "        result = \"NA\"\n",
    "    else:\n",
    "        try:\n",
    "            result = order_fix(parse(result).date(),valid_interval)\n",
    "        except:\n",
    "            # fail in parse(result)\n",
    "            status = \"BAD_PARSE\"\n",
    "        if not result:\n",
    "            status = \"BAD_PARSE\"\n",
    "    return result, status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_from_list(result):\n",
    "    \"\"\"Find and return the first date (if any) in a list of strings.\n",
    "    Used when a cell in a date column has a string with whitespaces.\n",
    "    Assumption: There is only one date in the list.\n",
    "    \"\"\"\n",
    "    segments = result.strip().split()\n",
    "    status = \"BAD_PARSE\"\n",
    "    if len(segments)>1: # 1 because length 0 is covered in parse_date     \n",
    "        for segment in segments:\n",
    "            d = list(datefinder.find_dates(segment.strip()))\n",
    "            if d:\n",
    "                result = d[0]\n",
    "                status = \"GOOD_PARSE\"\n",
    "                break\n",
    "    return result, status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(x,valid_interval):\n",
    "    \"\"\"Find and return a date in a cell.\n",
    "    Steps:\n",
    "        - Check if NA -> MISSING_VAL \n",
    "        - Check if date is in excel 5 digit format -> GOOD_PARSE or BAD_PARSE\n",
    "        - Check if datefinder can get date -> GOOD_PARSE or BAD_PARSE\n",
    "        - Check if string split on whitespace has dates -> GOOD_PARSE or BAD_PARSE\n",
    "    \"\"\"\n",
    "    valid_start = valid_interval[0]\n",
    "    valid_end = valid_interval[1]\n",
    "    status = \"GOOD_PARSE\"\n",
    "    if check_na(x):\n",
    "        return \"NA\", \"MISSING_VAL\"\n",
    "    result = str(x).strip()\n",
    "    if result.isnumeric() and 43000 < float(x) < 44500: # Impose loose limits on date\n",
    "        result = dt.date(1900, 1, 1) + timedelta(float(result)-2) # -2 required due to excel date peculiarity\n",
    "        if result < valid_start or result > valid_end:\n",
    "            status = \"BAD_PARSE\"\n",
    "    else:\n",
    "        try:\n",
    "            result = list(datefinder.find_dates(result.strip()))[0] \n",
    "        except:\n",
    "            status = \"BAD_PARSE\"\n",
    "        if status == \"BAD_PARSE\":\n",
    "            result, status = get_date_from_list(result)\n",
    "        if not result:\n",
    "            status = \"BAD_PARSE\"\n",
    "    return result, status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enum parsing\n",
    "# TO-DO: Set NA introduced by enum map as MISSING_VAL and not GOOD_PARSE - Done, test this functionality\n",
    "\n",
    "def parse_enum(x,enum_map):\n",
    "    status = \"GOOD_PARSE\"\n",
    "    if check_na(x):\n",
    "        return \"NA\", \"MISSING_VAL\"\n",
    "    result = x\n",
    "    result = str(result.strip()).lower()\n",
    "    if result in enum_map:\n",
    "        result = enum_map[result]\n",
    "        if result == \"na\":\n",
    "            result, status = \"NA\", \"MISSING_VAL\"\n",
    "    elif len(result) and result.split()[0] in enum_map:\n",
    "        result = enum_map[result.split()[0]]\n",
    "    elif len(result) and stem(result.split()[0]) in enum_map:\n",
    "        result = enum_map[stem(result.split()[0])]\n",
    "    else:\n",
    "        status = \"BAD_PARSE\"\n",
    "    return result, status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex parsing\n",
    "\n",
    "def parse_regex(x,regex_pat):\n",
    "    result = x\n",
    "    status =\"GOOD_PARSE\"\n",
    "    if check_na(x):\n",
    "        return \"NA\", \"MISSING_VAL\"\n",
    "    result = str(result)\n",
    "    if not re.match(regex_pat, result):\n",
    "        status = \"BAD_PARSE\"\n",
    "    return result, status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_df_and_mark_review(df,col,col_type,parse_args):\n",
    "    \"\"\"Parse a column and update the review columns in case there is a parsing issue\n",
    "        - Update review column (Not needed?)\n",
    "        - Update column value and individual review column status\n",
    "    \"\"\"\n",
    "    if (col_type =='regex'):\n",
    "        df['tmp_status'] = df[col].apply(lambda x: parse_regex(x,parse_args)[1])\n",
    "        df[col] = df[col].apply(lambda x: parse_regex(x,parse_args)[0])\n",
    "        df['Review'] = df.apply(lambda x: (x['Review'] == True or (x['tmp_status'] == 'BAD_PARSE')), axis=1)\n",
    "    elif (col_type =='enum'):\n",
    "        df['tmp_status'] = df[col].apply(lambda x: parse_enum(x,parse_args)[1])\n",
    "        df[col] = df[col].apply(lambda x: parse_enum(x,parse_args)[0])\n",
    "        df['Review'] = df.apply(lambda x: (x['Review'] == True or (x['tmp_status'] == 'BAD_PARSE')), axis=1)\n",
    "    elif (col_type =='date'):\n",
    "        df['tmp_status'] = df[col].apply(lambda x: parse_date(x,parse_args)[1])\n",
    "        df[col] = df[col].apply(lambda x: parse_date(x,parse_args)[0])\n",
    "    \n",
    "    df[col+'_Review'] = df['tmp_status']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression columns\n",
    "\n",
    "regex_columns =['Unique Code (Do not Fill)', 'ICMR ID', 'SNo',\n",
    "       'Patient ID/IPD ID', 'Name',  'Contact Number', 'Age'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date columns\n",
    "\n",
    "date_columns = [\n",
    "    'Date of Admission',\n",
    "    'Date of Outcome',\n",
    "    'Date of last test (to be left blank)',\n",
    "    'Dates of latest positive test (to be left blank)',\n",
    "    'Date of Sample Collection (FIRST)',\n",
    "    'Date of Sample Collection (Second)', \n",
    "    'Date of Sample Collection (Third)',\n",
    "    'Date of Sample Collection (Fourth)',\n",
    "    'Date of Sample Collection (Fifth)', \n",
    "    'Date of Sample Collection (Sixth)'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enum columns\n",
    "# TO-DO: Get maps for 'Symptomatic (Y/N)' and 'Current Health Status (Stable/Critical)' columns and add those\n",
    "\n",
    "enum_columns = [\n",
    "    'Sex (M/F)',\n",
    "    'Municipal Ward of patient Residence (to be filled by MCGM)',\n",
    "    'District of patient residence',\n",
    "    'Facility where admitted',\n",
    "    'Current Outcome (Admitted/\\n Discharge/\\n Death/\\n Transfer/\\n LAMA/\\n DAMA)',\n",
    "    'In case of Transfer, transferred to which hospital/Facility',\n",
    "    'Is patient in ICU? (Y/N)',\n",
    "    'If Critical mention intervention (Nasal O2, Facemask O2, HFNC, NRBM, NIV, Ventilator)',\n",
    "    'On dialysis (Y/N', \n",
    "    'Result of last test (to be left blank)',\n",
    "    'Sample Collected (Y/N)',\n",
    "    'Result of Sample (Positive/\\n Negative/\\n Awaited/\\n Inconclusive)',\n",
    "    'Result of Sample(second)',\n",
    "    'Result of Sample(Third)',\n",
    "    'Result of Sample (Fourth)',\n",
    "    'Result of Sample (Fifth)',\n",
    "    'Result of Sample (Sixth)'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text columns\n",
    "text_columns = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating enum maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the rough value lists for human map creation (This set of maps needs to be filled up manually)\n",
    "\n",
    "# for c in enum_columns:\n",
    "#     fname = \"\".join(c.split()) +\".csv\"\n",
    "#     fname=fname.replace('/','')\n",
    "#     tmp = D[c].apply(lambda x: str(x).lower())\n",
    "#     tmp = sorted(tmp.values)\n",
    "#     pd.DataFrame(set(tmp)).to_csv(fname,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_col_file_to_dict(infile):\n",
    "    df = pd.read_csv(infile,header=0)\n",
    "    d= pd.Series(df['Canonical'].values,index=df['Value']).to_dict()\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain info setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOMAIN INFO\n",
    "\n",
    "# Date - columns\n",
    "# TODO: future runs -end date will need to be modified \n",
    "# TODO: need additional logic later on the dates\n",
    "valid_date_interval={}\n",
    "for col in date_columns:\n",
    "    valid_date_interval[col] = [dt.date(2020,3,10), dt.date(2020,6,10)]\n",
    "\n",
    "# Enum - columns\n",
    "enum_map={}\n",
    "\n",
    "enum_map['Sex (M/F)']=two_col_file_to_dict(GENDER_LIST_FILE)\n",
    "enum_map['Municipal Ward of patient Residence (to be filled by MCGM)'] = two_col_file_to_dict(WARD_LIST_FILE)\n",
    "enum_map['District of patient residence'] = two_col_file_to_dict(DISTRICT_LIST_FILE)\n",
    "enum_map['Facility where admitted'] = two_col_file_to_dict(FACILITY_LIST_FILE)\n",
    "enum_map['Current Outcome (Admitted/\\n Discharge/\\n Death/\\n Transfer/\\n LAMA/\\n DAMA)']= two_col_file_to_dict(\n",
    "    OUTCOME_LIST_FILE)\n",
    "enum_map['Current Health Status (Stable/Critical)'] = two_col_file_to_dict(HSTATUS_LIST_FILE)\n",
    "enum_map['If Critical mention intervention (Nasal O2, Facemask O2, HFNC, NRBM, NIV, Ventilator)'] = two_col_file_to_dict(INTERVENTION_LIST_FILE)\n",
    "enum_map['In case of Transfer, transferred to which hospital/Facility'] = two_col_file_to_dict(FACILITY_LIST_FILE)\n",
    "enum_map['Result of last test (to be left blank)'] = two_col_file_to_dict(RESULT_STATUS_LIST_FILE)\n",
    "\n",
    "for col in ['Result of Sample (Positive/\\n Negative/\\n Awaited/\\n Inconclusive)','Result of Sample(second)',\n",
    "            'Result of Sample(Third)','Result of Sample (Fourth)','Result of Sample (Fifth)','Result of Sample (Sixth)']:\n",
    "    \n",
    "    enum_map[col]= two_col_file_to_dict(RESULT_STATUS_LIST_FILE)\n",
    "    \n",
    "for col in ['Symptomatic (Y/N)', 'Is patient in ICU? (Y/N)','On dialysis (Y/N', 'Sample Collected (Y/N)']:\n",
    "    enum_map[col]= two_col_file_to_dict(BOOL_LIST_FILE)\n",
    "\n",
    "# Regex - columns\n",
    "# TODO: Fix the patterns\n",
    "regex_pat={}\n",
    "regex_pat['Unique Code (Do not Fill)']= '([a-zA-Z]{1,2}[0-9]+)' \n",
    "regex_pat['ICMR ID']= '([0-9]+)'\n",
    "regex_pat['SNo']= '([0-9]+)' \n",
    "regex_pat['Patient ID/IPD ID']= '(.*?)'\n",
    "regex_pat['Name']= '(.*?)'\n",
    "regex_pat['Age']= '(.*?)'\n",
    "regex_pat['Contact Number']= '(.*?)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date columns\n",
    "for col in date_columns:\n",
    "    print(col)\n",
    "    D=parse_df_and_mark_review(D,col,'date',valid_date_interval[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# enum columns\n",
    "for col in enum_columns:\n",
    "    print(col)\n",
    "    D=parse_df_and_mark_review(D,col,'enum',enum_map[col])\n",
    "    print(D[col+'_Review'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex columns\n",
    "for col in regex_columns:\n",
    "    print(col)\n",
    "    D=parse_df_and_mark_review(D,col,'regex',regex_pat[col])\n",
    "    print(D[col+'_Review'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text columns ?\n",
    "# Do we need this or move symptomatic to enum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suspected dates are those for which interchanging the date and month gives a different valid date\n",
    "suspected_dates = set([(4, 5), (5, 4), (4, 6), (6, 4), (5, 6), (6, 5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_date(date, valid_interval):\n",
    "    \"\"\"\n",
    "    Flag missing and bad dates.\n",
    "    For remaining values:\n",
    "        - Set year as 2020.\n",
    "        - Check if suspected date.\n",
    "        - If date is invalid, check if invalidity can be corrected by swapping date and month.\n",
    "        (Necessary for cases where it is ambiguous whether to parse as mm/dd or dd/mm)\n",
    "    \"\"\"\n",
    "    valid_start = valid_interval[0]\n",
    "    valid_end = valid_interval[1]\n",
    "    status = \"GOOD_PARSE\"\n",
    "    if date == \"NA\":\n",
    "        status = \"MISSING_PARSE\"\n",
    "    elif not isinstance(date, dt.date): # and not isinstance(date, dt.datetime)\n",
    "        status = \"BAD_PARSE\"\n",
    "    else:\n",
    "        date = dt.date(2020, date.month, date.day)\n",
    "        if (date.day, date.month) in suspected_dates:\n",
    "            status = \"SUSPECTED_PARSE\"\n",
    "        elif ((date > valid_end) or (date < valid_start)):\n",
    "            try:\n",
    "                date = dt.date(2020, date.day, date.month)\n",
    "            except:\n",
    "                status = \"BAD_PARSE\"\n",
    "        else:\n",
    "            pass\n",
    "    return date, status\n",
    "\n",
    "def fix_dates(df, col, col_type, valid_interval):\n",
    "    df[col+'_Review'] = df[col].apply(lambda x: fix_date(x,valid_interval)[1])\n",
    "    df[col] = df[col].apply(lambda x: fix_date(x,valid_interval)[0])\n",
    "    \n",
    "    df['Review'] = df.apply(lambda x: (\n",
    "        (x['Review'] == True) or (x[col+'_Review'] == 'BAD_PARSE') or (x[col+'_Review'] == 'SUSPECTED_PARSE')), axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in date_columns:\n",
    "    print(col)\n",
    "    D=fix_dates(D,col,'date',valid_date_interval[col])\n",
    "    print(D[col+'_Review'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_col = []\n",
    "\n",
    "m = D.shape[0]\n",
    "n = D.shape[1]\n",
    "\n",
    "i = 0\n",
    "\n",
    "for i in range(m):\n",
    "    if not i%1000:\n",
    "        print(\"Processed \"+str(i))\n",
    "    suspect_flag = False\n",
    "    bad_flag = False\n",
    "    for j in range(40, n):\n",
    "        if D.iloc[i,j].startswith(\"BAD\"):\n",
    "            bad_flag = True\n",
    "        if D.iloc[i,j].startswith(\"SUSPECTED\"):\n",
    "            suspect_flag = True\n",
    "            break\n",
    "    if suspect_flag:\n",
    "        status_col.append(\"SUSPECTED\")\n",
    "    elif bad_flag:\n",
    "        status_col.append(\"BAD\")\n",
    "    else:\n",
    "        status_col.append(\"GOOD\")\n",
    "    i += 1\n",
    "        \n",
    "D['overall_status'] = status_col           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del D['tmp_status']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 500)\n",
    "D.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile value counts for review columns\n",
    "\n",
    "D_vc = D.iloc[:,40:]\n",
    "vc = D_vc.apply(lambda x: x.value_counts()).T.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts for review columns\n",
    "vc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestr = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "D_filename = \"\".join(['../data/consolidated_wards_clean_',timestr,'.csv'])\n",
    "# D.to_csv(D_filename, sep=',',index=False) # Copy of latest version \n",
    "D.to_csv('../data/consolidated_wards_clean.csv', sep=',',index=False) # Current working file for more processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the rough value lists for human map creation (To get any more values to map from rows with BAD_PARSE cells)\n",
    "\n",
    "# for c in enum_columns:\n",
    "#     fname = \"\".join(c.split()) +\".csv\"\n",
    "#     fname=fname.replace('/','')\n",
    "#     tmp = D.loc[D[c+'_Review'] == 'BAD_PARSE']\n",
    "#     tmp = tmp[c]\n",
    "#     print(tmp)\n",
    "#     tmp = tmp.apply(lambda x: str(x).lower())\n",
    "#     tmp = sorted(tmp.values)\n",
    "#     pd.DataFrame(set(tmp)).to_csv('csv/'+fname,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save value counts to file\n",
    "vc_filename = \"\".join(['../data/value_counts_',timestr,'.csv'])\n",
    "# vc.to_csv(vc_filename, header = ['Value Counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
